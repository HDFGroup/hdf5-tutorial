{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme\n",
    "\n",
    "**Table of contents:**\n",
    " - [Problem formulation](#Problem-formulation)\n",
    " - [Passing arguments to our programs](#Passing-arguments-to-our-programs)\n",
    " - [Creating sample paths](#Creating-sample-paths)\n",
    " - [Writing text files](#Writing-text-files)\n",
    " - [Writing binary files](#Writing-binary-files)\n",
    " - [Writing HDF5 files](#Writing-HDF5-files)\n",
    " - [Visualization](#Visualization)\n",
    " - [Discussion](#Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation\n",
    "\n",
    "Think of a device that over a certain epoch produces time series of measurements. The device could be a physical sensor or a numerical device such as a simulation, or it could be the price of a financial instrument such as a stock. Let's assume we want to capture such time series over many epochs and that each epoch has a fixed number of time steps. A visualization of five such series is shown in the figure (**Source:** __[Wikimedia](https://commons.wikimedia.org/wiki/File:Ornstein-Uhlenbeck-5traces.svg)__) below.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/Ornstein-Uhlenbeck-5traces.svg\" title=\"Five sampled traces of an Ornstein-Uhlenbeck process.\" />\n",
    "\n",
    "Assuming that an epoch of length 10 was sampled over 1,000 equidistant time steps, we could conveniently store the underlying data (values of `X`) in a `5 x 1000` two-dimensional array of floating-point values, with the series number and the time step as the row and column index, respectively.\n",
    "\n",
    "For our own benefit and the researchers' with whom we might want to share our observations, we should also record relevant information about the context in which the data was collected. This might be for example the unit of the quantity `X`, the length of an individual time step, the date, time, and location when the recording was made (if that were relvant), and any additional information about the calibration of the generating mechanism. To be specific, in this tutorial, we will use numerical samples of a well-known stochastic process, the so-called __[Ornsteinâ€“Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)__, as our data source. The sample traces or path samples shown in the figure were actually generated from a numerical experiment of such a process.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Up to you:</b> To follow this tutorial, it is <i>not</i> necessary to understand what a stochastic process is or how to simulate one. It is OK to think about it as a sophisticated pseudo-random number generator, which merely serves as our source of series data.\n",
    "</div>\n",
    "\n",
    "Our problem formulation is thus: Store the floating-point values of `path_count` series with `step_count` values each in a two-dimensional array of `path_count` rows and `step_count` columns. In addition, store the following floating-point calibrations, which are the same for all series\n",
    "- the time step `dt`\n",
    "- the long-term process mean `mu`\n",
    "- the reversion rate to the mean `theta`\n",
    "- the volatility of the process `sigma`.\n",
    "\n",
    "Before thinking about how to store this data, we will quickly go over how to pass the parameters to our simulation. After all, we want to control how much or how little data to generate! For completeness, we take a quick look at how the series data is generated. (It won't come as a shock that pseudo-random numbers play a big part!) With that out of the way, we look at storing the data in plain text files, as unformatted byte streams, and in HDF5 files. We conclude with a discussion of the advantages and challenges of the three approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing arguments to our programs\n",
    "\n",
    "We need two helper functions to pass and parse the relevant arguments to our programs.\n",
    "\n",
    "We use Pranav Srinivas Kumar's [`argparse` module](https://github.com/p-ranav/argparse), which is the \"gold standard\" in the C++ world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/parse_arguments.hpp\n",
    "#ifndef PARSE_ARGUMENTS_HPP\n",
    "#define PARSE_ARGUMENTS_HPP\n",
    "\n",
    "#include \"argparse.hpp\"\n",
    "\n",
    "// Sets the options for which we are looking\n",
    "extern void set_options(argparse::ArgumentParser& program);\n",
    "\n",
    "// Tests the options and retrieves the arguments\n",
    "extern int get_arguments\n",
    "(\n",
    "    const argparse::ArgumentParser& program,\n",
    "    size_t&                         path_count,\n",
    "    size_t&                         step_count,\n",
    "    double&                         dt,\n",
    "    double&                         theta,\n",
    "    double&                         mu,\n",
    "    double&                         sigma\n",
    ");\n",
    "\n",
    "#endif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of our functions is rather unsurprising and speaks to the quality (usability) of `argparse`.\n",
    "\n",
    "In `set_options()`, we inform `argparse` about the program options we want to support. For each argument, we tell `argparse` the short and long forms of the command line argument, a synopsis from which a help message will be created, and the default type and value of the argument. This information guides how `argparse` processes the `main` function's arguments, `argc` and `argv`.\n",
    "\n",
    "In `get_arguments()`, we check if an option was specificed and what value was provided, or use the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/parse_arguments.cpp\n",
    "\n",
    "#include \"parse_arguments.hpp\"\n",
    "#include <cfloat>\n",
    "#include <iostream>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "void set_options(argparse::ArgumentParser& program)\n",
    "{\n",
    "    program.add_argument(\"-p\", \"--paths\")  // command line arguments\n",
    "    .help(\"chooses the numnber of paths\")  // synopsis\n",
    "    .default_value(size_t{100})            // default value\n",
    "    .scan<'u', size_t>();                  // expected type\n",
    "\n",
    "    program.add_argument(\"-s\", \"--steps\")\n",
    "    .help(\"chooses the number of steps\")\n",
    "    .default_value(size_t{1000})\n",
    "    .scan<'u', size_t>();\n",
    "\n",
    "    program.add_argument(\"-d\", \"--dt\")\n",
    "    .help(\"chooses the time step\")\n",
    "    .default_value(double{0.01})\n",
    "    .scan<'f', double>();\n",
    "\n",
    "    program.add_argument(\"-t\", \"--theta\")\n",
    "    .help(\"chooses the rate of reversion to the mean\")\n",
    "    .default_value(double{1.0})\n",
    "    .scan<'f', double>();\n",
    "\n",
    "    program.add_argument(\"-m\", \"--mu\")\n",
    "    .help(\"chooses the long-term mean of the process\")\n",
    "    .default_value(double{0.0})\n",
    "    .scan<'f', double>();\n",
    "\n",
    "    program.add_argument(\"-g\", \"--sigma\")\n",
    "    .help(\"chooses the volatility of the process\")\n",
    "    .default_value(double{0.1})\n",
    "    .scan<'f', double>();\n",
    "}\n",
    "\n",
    "int get_arguments\n",
    "(\n",
    "    const argparse::ArgumentParser& program,\n",
    "    size_t&                         path_count,\n",
    "    size_t&                         step_count,\n",
    "    double&                         dt,\n",
    "    double&                         theta,\n",
    "    double&                         mu,\n",
    "    double&                         sigma\n",
    ")\n",
    "{\n",
    "    path_count = program.get<size_t>(\"--paths\");\n",
    "    if (path_count == 0) {\n",
    "        cerr << \"Number of paths must be greater than zero\" << endl;\n",
    "        return -1;\n",
    "    }\n",
    "    step_count = program.get<size_t>(\"--steps\");\n",
    "    if (step_count == 0) {\n",
    "        cerr << \"Number of steps must be greater than zero\" << endl;\n",
    "        return -1;\n",
    "    }\n",
    "    dt = program.get<double>(\"--dt\");\n",
    "    if (dt < DBL_MIN) {\n",
    "        cerr << \"Time step must be greater than zero\" << endl;\n",
    "        return -1;\n",
    "    }\n",
    "    theta = program.get<double>(\"--theta\");\n",
    "    if (theta < DBL_MIN) {\n",
    "        cerr << \"Reversion rate must be greater than zero\" << endl;\n",
    "        return -1;\n",
    "    }\n",
    "    mu = program.get<double>(\"--mu\");\n",
    "    sigma = program.get<double>(\"--sigma\");\n",
    "    if (sigma < DBL_MIN) {\n",
    "        cerr << \"Volatility must be greater than zero\" << endl;\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the compiler is happy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "g++ -std=c++17 -Wall -pedantic -I./include -c ./src/parse_arguments.cpp -o ./build/parse_arguments.o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sample paths\n",
    "\n",
    "We create `path_count` sample paths with `step_count` timesteps with the given parameters, and store them in a C++ `vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/ou_sampler.hpp\n",
    "#ifndef OU_SAMPLER_HPP\n",
    "#define OU_SAMPLER_HPP\n",
    "\n",
    "#include <vector>\n",
    "\n",
    "// Creates `path_count` sample paths of length `step_count` with parameters\n",
    "// `dt`, `theta`, `mu`, and `sigma`\n",
    "extern void ou_sampler\n",
    "(\n",
    "    std::vector<double>& ou_process,\n",
    "    const size_t&        path_count,\n",
    "    const size_t&        step_count,\n",
    "    const double&        dt,\n",
    "    const double&        theta,\n",
    "    const double&        mu,\n",
    "    const double&        sigma\n",
    ");\n",
    "\n",
    "#endif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/ou_sampler.cpp\n",
    "\n",
    "#include <random>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "void ou_sampler\n",
    "(\n",
    "    vector<double>& ou_process,\n",
    "    const size_t&   path_count,\n",
    "    const size_t&   step_count,\n",
    "    const double&   dt,\n",
    "    const double&   theta,\n",
    "    const double&   mu,\n",
    "    const double&   sigma\n",
    ")\n",
    "{\n",
    "    // Store sample paths in one contiguous buffer\n",
    "    ou_process.clear();\n",
    "    ou_process.resize(path_count * step_count);\n",
    "\n",
    "    random_device rd;\n",
    "    mt19937 generator(rd());\n",
    "    normal_distribution<double> dist(0.0, sqrt(dt));\n",
    "\n",
    "    for (size_t i = 0; i < path_count; ++i)\n",
    "    {\n",
    "        ou_process[i * step_count] = 0; // sample paths start at x = 0\n",
    "        for (size_t j = 1; j < step_count; ++j)\n",
    "        {\n",
    "            auto dW = dist(generator);\n",
    "            auto pos = i * step_count + j;\n",
    "            ou_process[pos] = ou_process[pos - 1] + theta * (mu - ou_process[pos - 1]) * dt + sigma * dW;\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "g++ -std=c++17 -Wall -pedantic -c ./src/ou_sampler.cpp -o ./build/ou_sampler.o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing text files\n",
    "\n",
    "The easiest way to store the sample paths would be a text file. We include a header in which we document the parameters that were uses in the generation. This is adequate as long as the number of sample paths and timesteps is relatively small. For large numbers, this is cleary cumbersome and unmanagable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/ou_text.cpp\n",
    "#include \"parse_arguments.hpp\"\n",
    "#include \"ou_sampler.hpp\"\n",
    "\n",
    "#include <fstream>\n",
    "#include <vector>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t path_count, step_count;\n",
    "    double dt, theta, mu, sigma;\n",
    "\n",
    "    argparse::ArgumentParser program(\"ou_text\");\n",
    "    set_options(program);\n",
    "    program.parse_args(argc, argv);\n",
    "    get_arguments(program, path_count, step_count, dt, theta, mu, sigma);\n",
    "\n",
    "    cout << \"Running with parameters:\"\n",
    "         << \" paths=\" << path_count << \" steps=\" << step_count\n",
    "         << \" dt=\" << dt << \" theta=\" << theta << \" mu=\" << mu << \" sigma=\" << sigma << endl;\n",
    "\n",
    "    vector<double> ou_process;\n",
    "    ou_sampler(ou_process, path_count, step_count, dt, theta, mu, sigma);\n",
    "    \n",
    "    // Write the sample paths to a text file\n",
    "    ofstream file(\"ou_process.txt\");\n",
    "    \n",
    "    file << \"# paths steps dt theta mu sigma\" << endl;\n",
    "    file << path_count << \" \" << step_count << \" \" << dt << \" \" << theta << \" \" << mu << \" \" << sigma << endl;\n",
    "    file << \"# data\" << endl;\n",
    "    \n",
    "    for (size_t i = 0; i < path_count; ++i)\n",
    "        {\n",
    "            for (size_t j = 0; j < step_count; ++j)\n",
    "            {\n",
    "                auto pos = i * step_count + j;\n",
    "                file << ou_process[pos] << \" \";\n",
    "            }\n",
    "            file << endl;\n",
    "        }\n",
    "\n",
    "    file.close();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "g++ -std=c++17 -Wall -pedantic -I./include ./src/ou_text.cpp ./build/parse_arguments.o ./build/ou_sampler.o -o ./build/ou_text\n",
    "./build/ou_text\n",
    "ls -iks ou_process.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing binary files\n",
    "\n",
    "To reduce the file size, instead of using text, we can store the sample paths as an unformatted binary stream. We would need to maintain separate documentation to tell consumers of the data how to parse and interpret this byte stream, which is a minor inconvenience. A major headache is that unformatted binary streams are platform (processor) dependent, which opens the door for misinterpretation when the data is copied to an incompatible platform and not adjusted for this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/ou_binary.cpp\n",
    "#include \"parse_arguments.hpp\"\n",
    "#include \"ou_sampler.hpp\"\n",
    "\n",
    "#include <fstream>\n",
    "#include <vector>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t path_count, step_count;\n",
    "    double dt, theta, mu, sigma;\n",
    "    \n",
    "    argparse::ArgumentParser program(\"ou_binary\");\n",
    "    set_options(program);\n",
    "    program.parse_args(argc, argv);\n",
    "    get_arguments(program, path_count, step_count, dt, theta, mu, sigma);\n",
    "\n",
    "    cout << \"Running with parameters:\"\n",
    "         << \" paths=\" << path_count << \" steps=\" << step_count\n",
    "         << \" dt=\" << dt << \" theta=\" << theta << \" mu=\" << mu << \" sigma=\" << sigma << endl;\n",
    "\n",
    "    vector<double> ou_process;\n",
    "    ou_sampler(ou_process, path_count, step_count, dt, theta, mu, sigma);\n",
    "    \n",
    "    // Write the sample paths to an unformatted binary file\n",
    "\n",
    "    ofstream file(\"ou_process.bin\", ios::out | ios::binary);\n",
    "    file.write((char *)&path_count, sizeof(path_count));\n",
    "    file.write((char *)&step_count, sizeof(step_count));\n",
    "    file.write((char *)&dt, sizeof(dt));\n",
    "    file.write((char *)&theta, sizeof(theta));\n",
    "    file.write((char *)&mu, sizeof(mu));\n",
    "    file.write((char *)&sigma, sizeof(sigma));\n",
    "    file.write((char *)ou_process.data(), sizeof(double) * ou_process.size());\n",
    "    file.close();\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "g++ -std=c++17 -Wall -pedantic -I./include ./src/ou_binary.cpp ./build/parse_arguments.o ./build/ou_sampler.o -o ./build/ou_binary\n",
    "./build/ou_binary\n",
    "ls -iks ou_process.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/ou_hdf5.cpp\n",
    "#include \"parse_arguments.hpp\"\n",
    "#include \"ou_sampler.hpp\"\n",
    "\n",
    "#include \"hdf5.h\"\n",
    "#include <vector>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "    size_t path_count, step_count;\n",
    "    double dt, theta, mu, sigma;\n",
    "\n",
    "    argparse::ArgumentParser program(\"ou_hdf5\");\n",
    "    set_options(program);\n",
    "    program.parse_args(argc, argv);\n",
    "    get_arguments(program, path_count, step_count, dt, theta, mu, sigma);\n",
    "\n",
    "    cout << \"Running with parameters:\"\n",
    "         << \" paths=\" << path_count << \" steps=\" << step_count\n",
    "         << \" dt=\" << dt << \" theta=\" << theta << \" mu=\" << mu << \" sigma=\" << sigma << endl;\n",
    "\n",
    "    vector<double> ou_process;\n",
    "    ou_sampler(ou_process, path_count, step_count, dt, theta, mu, sigma);\n",
    "    \n",
    "    //\n",
    "    // Write the sample paths to an HDF5 file using the HDF5 C-API!\n",
    "    //\n",
    "\n",
    "    auto file = H5Fcreate(\"ou_process.h5\", H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);\n",
    "\n",
    "    { // create & write the dataset\n",
    "        hsize_t dimsf[] = {(hsize_t)path_count, (hsize_t)step_count};\n",
    "        auto space = H5Screate_simple(2, dimsf, NULL);\n",
    "        auto dataset = H5Dcreate(file, \"/dataset\", H5T_NATIVE_DOUBLE, space, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);\n",
    "        H5Dwrite(dataset, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, ou_process.data());\n",
    "        H5Dclose(dataset);\n",
    "        H5Sclose(space);\n",
    "    }\n",
    "\n",
    "    { // make the file self-describing by adding a few attributes to `dataset`\n",
    "        auto scalar = H5Screate(H5S_SCALAR);\n",
    "        auto acpl = H5Pcreate(H5P_ATTRIBUTE_CREATE);\n",
    "        H5Pset_char_encoding(acpl, H5T_CSET_UTF8);\n",
    "        \n",
    "        auto set_attribute = [&](const string& name, const double& value) {\n",
    "            auto attr = H5Acreate_by_name(file, \"dataset\", name.c_str(), H5T_NATIVE_DOUBLE, scalar, acpl, H5P_DEFAULT, H5P_DEFAULT);\n",
    "            H5Awrite(attr, H5T_NATIVE_DOUBLE, &value);\n",
    "            H5Aclose(attr);\n",
    "        };\n",
    "        set_attribute(\"dt\", dt);\n",
    "        set_attribute(\"Î¸\", theta);\n",
    "        set_attribute(\"Î¼\", mu);\n",
    "        set_attribute(\"Ïƒ\", sigma);\n",
    "\n",
    "        H5Pclose(acpl);\n",
    "        H5Sclose(scalar);\n",
    "    }\n",
    "\n",
    "    H5Fclose(file);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "g++ -std=c++17 -Wall -pedantic -I/usr/include/hdf5/serial -L/usr/lib/x86_64-linux-gnu -I./include  ./src/ou_hdf5.cpp ./build/parse_arguments.o ./build/ou_sampler.o -o ./build/ou_hdf5 -lhdf5_serial\n",
    "./build/ou_hdf5\n",
    "ls -iks ou_process.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The easiest way to visualize the content of the HDF5 file `ou_process.h5` is to select it in the Explorer. This will launch the __[H5Web VS Code plugin](https://marketplace.visualstudio.com/items?itemName=h5web.vscode-h5web)__, which is sufficient for quick inspection and simple plot types.\n",
    "\n",
    "<img src=\"img/H5Web.png\" alt=\"H5Web\" title=\"H5Web VSCode plugin.\" />\n",
    "\n",
    "A more powerful option is __[Matplotlib](https://matplotlib.org/)__, and a plot of the fourty third sample path can be obtained as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "f = h5py.File(\"ou_process.h5\")\n",
    "dset = f[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = dset[42,:]\n",
    "print(f\"min: {arr.min():.2f}, max: {arr.max():.2f}, mean: {arr.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('_mpl-gallery')\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(0,len(arr)), arr, linewidth=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this part of the tutorial, we demonstrated how to store a set of sample paths plus metadata in plain text, as an unformatted binary stream, and in an HDF5 file. Advantages and challenges are relative to circumstances, and pointless to belabor in the abstract. If circumstances change, it might be time for a change of approach. Here are a few considerations:\n",
    "\n",
    "The simplicity of plain text is hard to beat for small data sets. No special tools or libraries are needed to handle text. (There are, however, __[challenges](https://youtu.be/_mZBa3sqTrI?si=sKQKvFgserRq2uHf)__ with \"plain text\" the moment you go beyond US-ASCII.) Performance in space (file size) and time (marshalling overhead) deteriorates with data size. Common mitigations include compression and parallel processing, but the original simplicity is gone.\n",
    "\n",
    "Unformatted binary streams are fine as long as platforms and layouts are stable. The moment multiple platforms with different byte orders enter the mix, things get tricky. Change in the metadata or data is the biggest enemy. Not only is (separate) documentation in need of updates, but costly code changes to support legacy and new data might be necessary.\n",
    "\n",
    "The HDF5 framework takes care of all of the issues mentioned so far at the expense of a dependency on the HDF5 library and file format or HSDS. Common considerations before deciding on making your product dependent on the HDF5 framework include:\n",
    "\n",
    "1. **Licensing and Costs:** The HDF5 framework is open source and released under a __[permissive license](https://www.hdfgroup.org/licenses/)__, which allows for free use, modification, and redistribution of the software, both in source and binary forms, with minimal restrictions. As open-source software, the HDF5 framework is generally free to use. There are no costs associated with downloading and using the HDF5 library or HSDS in your projects. However, there might be costs associated with specific third-party tools or commercial applications that use HDF5 as part of their software.\n",
    "2. **Compatibility and Integration:** The HDF5 framework is designed to be highly portable and runs on a wide range of hardware and operating system platforms, including Linux, macOS, and Windows. HDF5 is supported in various programming languages, including C, C++, Fortran, Java, Python, and many more through official and third-party bindings. The HDF5 data model is unique, and frequently copied in parts. If you are integrating HDF5 into an existing system, consider how HDF5's primitives (datasets, groups, attributes, etc.) map to your applicationâ€™s data model.\n",
    "3. **Support and Documentation:** The support and documentation for HDF5 are robust, catering to a wide range of users from beginners to advanced. The HDF Group provides extensive official documentation. This includes detailed guides on installation, API references, user guides for various programming languages, and explanations of HDF5 concepts. The documentation is regularly updated to reflect the latest developments. The HDF5 framework has a large and active user community. Community support is available through mailing lists, forums, and Q&A sites where users can ask questions and share knowledge. The HDF Forum is particularly active and a good resource for seeking help and advice. There are numerous tutorials, examples, and use cases available, both within the official documentation and from third-party sources. These resources are valuable for both new and experienced users in understanding how to effectively use HDF5 for different applications. The HDF Group offers __[commercial support](https://www.hdfgroup.org/solutions/priority-support/)__ and __[consulting services](https://www.hdfgroup.org/solutions/consulting/)__. This includes help with installation, optimization, data modeling, and other technical issues.\n",
    "4. **Reliability and Performance:** The HDF5 framework is known for its reliability and performance, particularly in environments that require handling large and complex datasets. However, like any software, its performance and reliability can depend on various factors including usage patterns, system architecture, and specific application requirements. As a mature and widely-used technology, HDF5 is generally considered stable. It is used in critical applications across different domains and industries, which speaks to its reliability. For applications requiring parallel processing, HDF5 can be used with MPI (Message Passing Interface) to enable efficient parallel I/O operations. HDF5 allows users to tailor data storage layouts to match specific access patterns, which can greatly optimize read/write performance. HDF5 supports various data compression techniques, which can reduce file size and I/O overhead, especially beneficial for network transfers and storage efficiency.\n",
    "5. **Updates and Longevity:** The HDF Group regularly releases updates. These updates include new features, performance improvements, bug fixes, and security patches. The HDF Group has a strong commitment to backward and forward compatibility. This means that newer versions of the HDF5 framework maintain compatibility with older versions, which is critical for long-term data accessibility. At the same time, older versions of the framework can access data stored using a newer version as long as no features available only in later versions were used in storing the data. (**Fun fact:** On 6 November 1998, version 1.0.0 of the HDF5 library and file format was released.)\n",
    "6. **Community and Ecosystem:** The HDF5community and ecosystem are notable for their diversity, collaboration, and continuous growth, reflecting the wide range of applications and industries that utilize the HDF5 framework. HDF5 is used across various sectors including academic research, government agencies, high-performance computing, aerospace, finance, and environmental science. This diversity leads to a rich community with a wide array of use cases and expertise. Users and developers actively engage through mailing lists, forums, and social media platforms. The HDF Forum is particularly notable for its active discussions and exchange of knowledge. The HDF Group, which oversees the development of HDF5, often collaborates with the user community in identifying priorities, improving features, and fixing bugs. This collaborative approach ensures that HDF5 continues to meet the evolving needs of its users. The HDF5 community is global, with users and contributors from around the world. This international aspect brings diverse perspectives and use cases to the community. There is a wealth of educational resources available, including tutorials, webinars, workshops, and documentation, which help new users learn about HDF5 and assist experienced users in tackling more complex tasks. Regular conferences, workshops, and webinars are held, often organized by the HDF Group or in conjunction with scientific conferences, fostering community interaction and knowledge sharing. Being open source, HDF5 benefits from contributions from individuals and organizations worldwide. This includes code contributions, bug reports, and feature requests. The HDF Group often partners with academic institutions, research organizations, and industry leaders on various projects, ensuring that HDF5 continues to align with cutting-edge data needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdf5-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
