{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0bf0f0a-b19e-4a50-8a13-820ec7439437",
   "metadata": {},
   "source": [
    "## Read Directly from the Cloud\n",
    "\n",
    "In the previous notebook, we showed how the library can use a VOL connector to access files stored in a cloud backend through HSDS. But this is not the only way to use HDF5 in the cloud. HDF5 files stored in an s3 bucket can be accessed directly from the library without any external connector or drivers. This is possible through the Read-Only s3 VFD (ROS3 VFD). \n",
    "\n",
    "### Enabling the ROS3 VFD\n",
    "\n",
    "The ROS3 VFD is packaged with the library, and only needs to be enabled during the build process. When using CMake to build HDF5, this is done by setting the variable `HDF5_ENABLE_ROS3_VFD` to `ON`.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "The cloning and compilation takes, depending on the machine type, about three minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7406c3-96f8-4a65-8cdc-69090897543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'build/hdf5' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/HDFGroup/hdf5.git build/hdf5\n",
    "mkdir -p build/hdf5/build\n",
    "cd build/hdf5/build\n",
    "cmake -DCMAKE_INSTALL_PREFIX=/home/vscode/.local -DHDF5_ENABLE_ROS3_VFD=ON -DBUILD_STATIC_LIBS=OFF -DBUILD_TESTING=OFF -DHDF5_BUILD_EXAMPLES=OFF -DHDF5_BUILD_TOOLS=ON -DHDF5_BUILD_UTILS=OFF ../ 2>&1 > /dev/null\n",
    "make -j 4 2>&1 > /dev/null\n",
    "make install 2>&1 > /dev/null\n",
    "cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa2db2-6da3-4057-ba43-5b5f904cdc9d",
   "metadata": {},
   "source": [
    "For this demonstration, we will use a copy of the `ou_process.h5` file that has been uploaded to an s3 bucket. We can use the command line tool `h5dump` to examine this file and see that it is the same file we're familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "082b1665-2780-4d45-9cc0-9e72aec7a207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 \"https://s3.us-east-2.amazonaws.com/docs.hdfgroup.org/hdf5/h5/ou_process.h5\" {\n",
      "SUPER_BLOCK {\n",
      "   SUPERBLOCK_VERSION 0\n",
      "   FREELIST_VERSION 0\n",
      "   SYMBOLTABLE_VERSION 0\n",
      "   OBJECTHEADER_VERSION 0\n",
      "   OFFSET_SIZE 8\n",
      "   LENGTH_SIZE 8\n",
      "   BTREE_RANK 16\n",
      "   BTREE_LEAF 4\n",
      "   ISTORE_K 32\n",
      "   FILE_SPACE_STRATEGY H5F_FSPACE_STRATEGY_FSM_AGGR\n",
      "   FREE_SPACE_PERSIST FALSE\n",
      "   FREE_SPACE_SECTION_THRESHOLD 1\n",
      "   FILE_SPACE_PAGE_SIZE 4096\n",
      "   USER_BLOCK {\n",
      "      USERBLOCK_SIZE 0\n",
      "   }\n",
      "}\n",
      "GROUP \"/\" {\n",
      "   ATTRIBUTE \"source\" {\n",
      "      DATATYPE  H5T_STRING {\n",
      "         STRSIZE 41;\n",
      "         STRPAD H5T_STR_NULLTERM;\n",
      "         CSET H5T_CSET_ASCII;\n",
      "         CTYPE H5T_C_S1;\n",
      "      }\n",
      "      DATASPACE  SCALAR\n",
      "   }\n",
      "   DATASET \"dataset\" {\n",
      "      DATATYPE  H5T_IEEE_F64LE\n",
      "      DATASPACE  SIMPLE { ( 100, 1000 ) / ( 100, 1000 ) }\n",
      "      STORAGE_LAYOUT {\n",
      "         CONTIGUOUS\n",
      "         SIZE 800000\n",
      "         OFFSET 2048\n",
      "      }\n",
      "      FILTERS {\n",
      "         NONE\n",
      "      }\n",
      "      FILLVALUE {\n",
      "         FILL_TIME H5D_FILL_TIME_IFSET\n",
      "         VALUE  H5D_FILL_VALUE_DEFAULT\n",
      "      }\n",
      "      ALLOCATION_TIME {\n",
      "         H5D_ALLOC_TIME_LATE\n",
      "      }\n",
      "      ATTRIBUTE \"Wikipedia\" {\n",
      "         DATATYPE  H5T_STRING {\n",
      "            STRSIZE 64;\n",
      "            STRPAD H5T_STR_NULLTERM;\n",
      "            CSET H5T_CSET_ASCII;\n",
      "            CTYPE H5T_C_S1;\n",
      "         }\n",
      "         DATASPACE  SCALAR\n",
      "      }\n",
      "      ATTRIBUTE \"columns\" {\n",
      "         DATATYPE  H5T_STRING {\n",
      "            STRSIZE 4;\n",
      "            STRPAD H5T_STR_NULLTERM;\n",
      "            CSET H5T_CSET_ASCII;\n",
      "            CTYPE H5T_C_S1;\n",
      "         }\n",
      "         DATASPACE  SCALAR\n",
      "      }\n",
      "      ATTRIBUTE \"comment\" {\n",
      "         DATATYPE  H5T_STRING {\n",
      "            STRSIZE 68;\n",
      "            STRPAD H5T_STR_NULLTERM;\n",
      "            CSET H5T_CSET_ASCII;\n",
      "            CTYPE H5T_C_S1;\n",
      "         }\n",
      "         DATASPACE  SCALAR\n",
      "      }\n",
      "      ATTRIBUTE \"dt\" {\n",
      "         DATATYPE  H5T_IEEE_F64LE\n",
      "         DATASPACE  SCALAR\n",
      "      }\n",
      "      ATTRIBUTE \"rows\" {\n",
      "         DATATYPE  H5T_STRING {\n",
      "            STRSIZE 4;\n",
      "            STRPAD H5T_STR_NULLTERM;\n",
      "            CSET H5T_CSET_ASCII;\n",
      "            CTYPE H5T_C_S1;\n",
      "         }\n",
      "         DATASPACE  SCALAR\n",
      "      }\n",
      "      ATTRIBUTE \"θ\" {\n",
      "         DATATYPE  H5T_IEEE_F64LE\n",
      "         DATASPACE  SCALAR\n",
      "      }\n",
      "      ATTRIBUTE \"μ\" {\n",
      "         DATATYPE  H5T_IEEE_F64LE\n",
      "         DATASPACE  SCALAR\n",
      "      }\n",
      "      ATTRIBUTE \"σ\" {\n",
      "         DATATYPE  H5T_IEEE_F64LE\n",
      "         DATASPACE  SCALAR\n",
      "      }\n",
      "   }\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/home/vscode/.local/bin/h5dump-shared -pBH --vfd-name ros3 --s3-cred=\"(,,)\" https://s3.us-east-2.amazonaws.com/docs.hdfgroup.org/hdf5/h5/ou_process.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d3599-4f70-41d4-aff4-05842fd14efd",
   "metadata": {},
   "source": [
    "Now we will use the ROS3 VFD to read the file. Similarly to other VFDs, the ROS3 VFD is enabled at file access time with a File Access Property List (FAPL) modified via `H5Pset_fapl_ros3()`. This function also requires some s3-specific parameters in the form of the `H5FD_ros3_fapl_t` structure. Because this file is publicly accessible without any authentication, all we need to provide is the region it is located in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bd871fd-7521-4d58-b234-b7627a292b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ou_ros3.c\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ou_ros3.c\n",
    "#include \"hdf5.h\"\n",
    "#include <stdlib.h>\n",
    "#include <string.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "hid_t file_id;\n",
    "hid_t fapl_id;\n",
    "hid_t dset_id;\n",
    "hid_t dspace_id;\n",
    "hid_t dtype_id;\n",
    "hid_t attr_id;\n",
    "\n",
    "int rank = 2;\n",
    "const hsize_t dims[] = {100, 1000};\n",
    "double *read_buffer = (double*) calloc(dims[0] * dims[1], sizeof(double));\n",
    "\n",
    "H5FD_ros3_fapl_t s3_params;\n",
    "\n",
    "const char* aws_region = \"us-east-2\";\n",
    "strcpy(s3_params.aws_region, aws_region);\n",
    "\n",
    "const char* file_uri = \"https://s3.us-east-2.amazonaws.com/docs.hdfgroup.org/hdf5/h5/ou_process.h5\";\n",
    "\n",
    "/* The version of this ROS3 FAPL parameter structure */\n",
    "s3_params.version = 1;\n",
    "\n",
    "/* This file permits anonymous access, so authentication is not needed */\n",
    "/* secret_id and secret_key auth fields that would be required for protected s3 buckets */\n",
    "s3_params.authenticate = 0; \n",
    "\n",
    "fapl_id = H5Pcreate(H5P_FILE_ACCESS);\n",
    "\n",
    "H5Pset_fapl_ros3(fapl_id, &s3_params);\n",
    "\n",
    "/* Open and read from the file */\n",
    "file_id = H5Fopen(file_uri, H5F_ACC_RDONLY, fapl_id);\n",
    "\n",
    "dset_id = H5Dopen(file_id, \"dataset\", H5P_DEFAULT);\n",
    "\n",
    "dspace_id = H5Screate_simple(rank, dims, NULL);\n",
    "\n",
    "/* Read data */\n",
    "H5Dread(dset_id, H5T_IEEE_F64LE, H5S_ALL, H5S_ALL, H5P_DEFAULT, (void*) read_buffer);\n",
    "\n",
    "for (size_t i = 0; i < 10; i++) {\n",
    "    size_t row_idx = i * dims[0] / 10;\n",
    "    size_t col_idx = i * dims[1] / 10;\n",
    "    double element = read_buffer[row_idx * dims[1] + col_idx];\n",
    "    printf(\"Element at index (%zu, %zu) = %lf\\n\", row_idx, col_idx, element);\n",
    "}\n",
    "\n",
    "free(read_buffer);\n",
    "H5Dclose(dset_id);\n",
    "H5Sclose(dspace_id);\n",
    "H5Pclose(fapl_id);\n",
    "H5Fclose(file_id);\n",
    "return 0;\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8b45b90-3ec9-4af6-8536-256eb4c06bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element at index (0, 0) = 0.000000\n",
      "Element at index (10, 100) = 0.077412\n",
      "Element at index (20, 200) = 0.031822\n",
      "Element at index (30, 300) = 0.050663\n",
      "Element at index (40, 400) = 0.076260\n",
      "Element at index (50, 500) = 0.096194\n",
      "Element at index (60, 600) = 0.013299\n",
      "Element at index (70, 700) = -0.030038\n",
      "Element at index (80, 800) = 0.048959\n",
      "Element at index (90, 900) = -0.070738\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "g++ -I/home/vscode/.local/include -L/home/vscode/.local/lib ./src/ou_ros3.c -o ./build/ou_ros3 -lhdf5 -Wl,-rpath=/home/vscode/.local/lib\n",
    "./build/ou_ros3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0351df0-5336-416d-86e3-c40ee8a1b1c6",
   "metadata": {},
   "source": [
    "The ROS3 VFD is also transparently accessible through h5py! (remember Tutorial 04?) The default installation of h5py does not have th ROS3 VFD enabled. To enable it, we build h5py from source against an HDF5 library that already has the ROS3 VFD enabled. Then, we can access our file in the cloud by providing the same ROS3 arguments as before to `h5py.File()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8da438a-7df4-4860-90af-e031f86fed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'h5py' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/matthewlarson/Documents/hdf5-tutorial/h5py\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/matthewlarson/Documents/anaconda3/envs/new_env2/lib/python3.9/site-packages (from h5py==3.10.0) (1.26.1)\n",
      "Building wheels for collected packages: h5py\n",
      "  Building wheel for h5py (pyproject.toml): started\n",
      "  Building wheel for h5py (pyproject.toml): still running...\n",
      "  Building wheel for h5py (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for h5py: filename=h5py-3.10.0-cp39-cp39-linux_x86_64.whl size=1874495 sha256=bf5eecaf84e283629c2ee4e2efdca2abefd21523c217410c175249745ffac61a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yvc30wut/wheels/72/49/cf/e202bdbdc4979200d58297536c81c24a0fe7cadbdc685820b6\n",
      "Successfully built h5py\n",
      "Installing collected packages: h5py\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.10.0\n",
      "    Uninstalling h5py-3.10.0:\n",
      "      Successfully uninstalled h5py-3.10.0\n",
      "Successfully installed h5py-3.10.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/h5py/h5py\n",
    "cd h5py\n",
    "HDF5_DIR=/home/vscode/.local pip install .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70bbf3d-5cd3-41fb-820d-dad47a7a41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset[0, 0] = 0.0\n",
      "dataset[10, 100] = 0.07741231848747207\n",
      "dataset[20, 200] = 0.03182172849584258\n",
      "dataset[30, 300] = 0.050662851693772944\n",
      "dataset[40, 400] = 0.07626000210825314\n",
      "dataset[50, 500] = 0.096193561713216\n",
      "dataset[60, 600] = 0.01329928934495046\n",
      "dataset[70, 700] = -0.03003752756107992\n",
      "dataset[80, 800] = 0.048958998265666936\n",
      "dataset[90, 900] = -0.07073801698079688\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "dims = [100, 1000]\n",
    "\n",
    "s3_uri = \"https://s3.us-east-2.amazonaws.com/docs.hdfgroup.org/hdf5/h5/ou_process.h5\"\n",
    "kwargs = {}\n",
    "kwargs['mode'] = 'r'\n",
    "kwargs['driver'] = \"ros3\"\n",
    "kwargs['aws_region'] = (\"us-east-2\").encode(\"utf-8\")\n",
    "# kwargs['authenticate'] = 0\n",
    "\n",
    "f = h5py.File(s3_uri, **kwargs)\n",
    "\n",
    "dset = f[\"dataset\"]\n",
    "\n",
    "data = dset[:]\n",
    "\n",
    "for i in range(10):\n",
    "    row_idx = int(i * dims[0] / 10)\n",
    "    col_idx = int(i * dims[1] / 10)\n",
    "    print(f\"dataset[{row_idx}, {col_idx}] = {data[row_idx, col_idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6994af-1120-4870-9c5d-c2e7c4bf0df8",
   "metadata": {},
   "source": [
    "## Cloud-Optimized\n",
    "\n",
    "While it is possible to read HDF5 files in their native format directly from the Cloud, this introduces some inefficiencies. The access patterns of the library were optimized for filesystem I/O, not cloud access and the delays it brings. These concerns motivated the development of the [HDF5 Cloud-Optimized Read-Only Library (H5Coro)](https://github.com/ICESat2-SlideRule/h5coro). H5Coro a pure Python implementation of a subset of the HDF5 specification that has been optimized for reading data out of S3. For large files, H5Coro can speed up access times by [two orders of magnitude](https://www.hdfgroup.org/wp-content/uploads/2021/05/JPSwinski_H5Coro.pdf). This is achieved primarily through using a larger cache to avoid repeated small reads to the cloud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
